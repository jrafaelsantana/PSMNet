{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from dataloader import KITTILoader as DA\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"PSMNet\")\n",
    "\n",
    "parser.add_argument(\"--maxdisp\", type=int, default=192, help=\"maxium disparity\")\n",
    "parser.add_argument(\"--model\", default=\"stackhourglass\", help=\"select model\")\n",
    "parser.add_argument(\"--datatype\", default=\"2015\", help=\"datapath\")\n",
    "parser.add_argument(\"--datapath\", default=\"./dataset/training/\", help=\"datapath\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=300, help=\"number of epochs to train\")\n",
    "parser.add_argument(\"--loadmodel\", default=None, help=\"load model\")\n",
    "parser.add_argument(\"--savemodel\", default=\"./\", help=\"save model\")\n",
    "parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False, help=\"enable CUDA\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "if args.datatype == \"2015\":\n",
    "    from dataloader import KITTIloader2015 as ls\n",
    "elif args.datatype == \"2012\":\n",
    "    from dataloader import KITTIloader2012 as ls\n",
    "\n",
    "(\n",
    "    all_left_img,\n",
    "    all_right_img,\n",
    "    all_left_disp,\n",
    "    test_left_img,\n",
    "    test_right_img,\n",
    "    test_left_disp,\n",
    ") = ls.dataloader(args.datapath)\n",
    "\n",
    "TrainImgLoader = torch.utils.data.DataLoader(\n",
    "    DA.myImageFloder(all_left_img, all_right_img, all_left_disp, True),\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "TestImgLoader = torch.utils.data.DataLoader(\n",
    "    DA.myImageFloder(test_left_img, test_right_img, test_left_disp, False),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model == \"stackhourglass\":\n",
    "    model = stackhourglass(args.maxdisp)\n",
    "elif args.model == \"basic\":\n",
    "    model = basic(args.maxdisp)\n",
    "else:\n",
    "    print(\"no model\")\n",
    "\n",
    "if args.cuda:\n",
    "    model = nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "\n",
    "if args.loadmodel is not None:\n",
    "    state_dict = torch.load(args.loadmodel)\n",
    "    model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "print(\n",
    "    \"Number of model parameters: {}\".format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.1, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(imgL, imgR, disp_L):\n",
    "    model.train()\n",
    "    imgL = Variable(torch.FloatTensor(imgL))\n",
    "    imgR = Variable(torch.FloatTensor(imgR))\n",
    "    disp_L = Variable(torch.FloatTensor(disp_L))\n",
    "\n",
    "    if args.cuda:\n",
    "        imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n",
    "\n",
    "    # ---------\n",
    "    mask = disp_true > 0\n",
    "    mask.detach_()\n",
    "    # ----\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if args.model == \"stackhourglass\":\n",
    "        output1, output2, output3 = model(imgL, imgR)\n",
    "        output1 = torch.squeeze(output1, 1)\n",
    "        output2 = torch.squeeze(output2, 1)\n",
    "        output3 = torch.squeeze(output3, 1)\n",
    "        loss = (\n",
    "            0.5 * F.smooth_l1_loss(output1[mask], disp_true[mask], size_average=True)\n",
    "            + 0.7 * F.smooth_l1_loss(output2[mask], disp_true[mask], size_average=True)\n",
    "            + F.smooth_l1_loss(output3[mask], disp_true[mask], size_average=True)\n",
    "        )\n",
    "    elif args.model == \"basic\":\n",
    "        output = model(imgL, imgR)\n",
    "        output = torch.squeeze(output3, 1)\n",
    "        loss = F.smooth_l1_loss(output3[mask], disp_true[mask], size_average=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(imgL, imgR, disp_true):\n",
    "    model.eval()\n",
    "    imgL = Variable(torch.FloatTensor(imgL))\n",
    "    imgR = Variable(torch.FloatTensor(imgR))\n",
    "    if args.cuda:\n",
    "        imgL, imgR = imgL.cuda(), imgR.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output3 = model(imgL, imgR)\n",
    "\n",
    "    pred_disp = output3.data.cpu()\n",
    "\n",
    "    # computing 3-px error#\n",
    "    true_disp = copy.deepcopy(disp_true)\n",
    "    index = np.argwhere(true_disp > 0)\n",
    "    disp_true[index[0][:], index[1][:], index[2][:]] = np.abs(\n",
    "        true_disp[index[0][:], index[1][:], index[2][:]]\n",
    "        - pred_disp[index[0][:], index[1][:], index[2][:]]\n",
    "    )\n",
    "    correct = (disp_true[index[0][:], index[1][:], index[2][:]] < 3) | (\n",
    "        disp_true[index[0][:], index[1][:], index[2][:]]\n",
    "        < true_disp[index[0][:], index[1][:], index[2][:]] * 0.05\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return 1 - (float(torch.sum(correct)) / float(len(index[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    if epoch <= 200:\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        lr = 0.0001\n",
    "    print(lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    max_acc = 0\n",
    "    max_epo = 0\n",
    "    start_full_time = time.time()\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        ## training ##\n",
    "        for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            loss = train(imgL_crop, imgR_crop, disp_crop_L)\n",
    "            print(\n",
    "                \"Iter %d training loss = %.3f , time = %.2f\"\n",
    "                % (batch_idx, loss, time.time() - start_time)\n",
    "            )\n",
    "            total_train_loss += loss\n",
    "\n",
    "        print(\n",
    "            \"epoch %d total training loss = %.3f\"\n",
    "            % (epoch, total_train_loss / len(TrainImgLoader))\n",
    "        )\n",
    "\n",
    "        ## Test ##\n",
    "        for batch_idx, (imgL, imgR, disp_L) in enumerate(TestImgLoader):\n",
    "            test_loss = test(imgL, imgR, disp_L)\n",
    "            print(\"Iter %d 3-px error in val = %.3f\" % (batch_idx, test_loss * 100))\n",
    "            total_test_loss += test_loss\n",
    "\n",
    "        print(\n",
    "            \"epoch %d total 3-px error in val = %.3f\"\n",
    "            % (epoch, total_test_loss / len(TestImgLoader) * 100)\n",
    "        )\n",
    "        if total_test_loss / len(TestImgLoader) * 100 > max_acc:\n",
    "            max_acc = total_test_loss / len(TestImgLoader) * 100\n",
    "            max_epo = epoch\n",
    "\n",
    "        print(\"MAX epoch %d total test error = %.3f\" % (max_epo, max_acc))\n",
    "\n",
    "        # SAVE\n",
    "        savefilename = args.savemodel + \"finetune_\" + str(epoch) + \".tar\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"train_loss\": total_train_loss / len(TrainImgLoader),\n",
    "                \"test_loss\": total_test_loss / len(TestImgLoader) * 100,\n",
    "            },\n",
    "            savefilename,\n",
    "        )\n",
    "\n",
    "        print(\"full finetune time = %.2f HR\" % ((time.time() - start_full_time) / 3600))\n",
    "    print(max_epo)\n",
    "    print(max_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
